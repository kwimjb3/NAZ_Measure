{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ef5e8a2-ab1e-44fc-91aa-a4c227953a77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Variable Timing Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect repo root\n",
    "repo_root = Path(\"/workspaces/NAZ_Measure\")  # absolute path to your repo root\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Now imports will work\n",
    "from configs.shuffleboard_2025_config import start_date, end_date\n",
    "from utils.data import get_raw_data_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Builder' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatabricks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatabricksSession\n\u001b[32m      3\u001b[39m spark = \u001b[43mDatabricksSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<host>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<token>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mclusterId\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<cluster>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mspark.driver.memory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m512m\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      8\u001b[39m     .config(\u001b[33m\"\u001b[39m\u001b[33mspark.executor.memory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m512m\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      9\u001b[39m     .getOrCreate()\n",
      "\u001b[31mAttributeError\u001b[39m: 'Builder' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "spark = DatabricksSession.builder \\\n",
    "    .host(\"<host>\") \\\n",
    "    .token(\"<token>\") \\\n",
    "    .clusterId(\"<cluster>\") \\\n",
    "    .config(\"spark.driver.memory\", \"512m\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/NAZ_Measure/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "pyspark and databricks-connect cannot be installed at the same time. To use databricks-connect, uninstall databricks-connect & pyspark by running 'pip uninstall -y databricks-connect pyspark pyspark-connect pyspark-client' followed by a re-installation of databricks-connect",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatabricks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatabricksSession\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# This uses the environment variables from devcontainer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m spark = \u001b[43mDatabricksSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(spark.version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/NAZ_Measure/.venv/lib/python3.12/site-packages/databricks/connect/session.py:402\u001b[39m, in \u001b[36mDatabricksSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetOrCreate\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> SparkSession:\n\u001b[32m    383\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    384\u001b[39m \u001b[33;03m    Get an existing :class:SparkSession for the provided configuration or, if there is no\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[33;03m     existing one, create a new one.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    400\u001b[39m \u001b[33;03m        queries to this spark session are executed remotely.\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/NAZ_Measure/.venv/lib/python3.12/site-packages/databricks/connect/session.py:521\u001b[39m, in \u001b[36mDatabricksSession.Builder._create\u001b[39m\u001b[34m(self, _Builder__skip_cache)\u001b[39m\n\u001b[32m    519\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mFalling back to default configuration from the SDK.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    520\u001b[39m config = Config()\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_sdkconfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gen_user_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_session_enabled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__skip_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__skip_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/NAZ_Measure/.venv/lib/python3.12/site-packages/databricks/connect/cache.py:56\u001b[39m, in \u001b[36mcached.<locals>._cached.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m __skip_cache \u001b[38;5;129;01mor\u001b[39;00m cache_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cache \u001b[38;5;129;01mor\u001b[39;00m is_stale(cache[cache_id]):\n\u001b[32m     55\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCaching: creating a new session.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     cache[cache_id] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     58\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mCaching: reusing existing session.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/NAZ_Measure/.venv/lib/python3.12/site-packages/databricks/connect/session.py:638\u001b[39m, in \u001b[36mDatabricksSession.Builder._from_sdkconfig\u001b[39m\u001b[34m(config, user_agent, headers, validate_session, env, **kwargs)\u001b[39m\n\u001b[32m    636\u001b[39m         validate_session_serverless(session)\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m         \u001b[43mvalidate_session_with_sdk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/NAZ_Measure/.venv/lib/python3.12/site-packages/databricks/connect/validation.py:100\u001b[39m, in \u001b[36mvalidate_session_with_sdk\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m     93\u001b[39m     displayed_dbr_version = \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, dbr_version)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dbr_version) > \u001b[32m1\u001b[39m \\\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(dbr_version[\u001b[32m0\u001b[39m]) + \u001b[33m\"\u001b[39m\u001b[33m.x\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsupported combination of Databricks Runtime & \u001b[39m\u001b[33m\"\u001b[39m +\n\u001b[32m     96\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mDatabricks Connect versions: \u001b[39m\u001b[33m\"\u001b[39m +\n\u001b[32m     97\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_dbr_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (Databricks Runtime) \u001b[39m\u001b[33m\"\u001b[39m +\n\u001b[32m     98\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m< \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_connect_version.base_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (Databricks Connect).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[43m_validate_pyspark_installation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mSession validated successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/NAZ_Measure/.venv/lib/python3.12/site-packages/databricks/connect/validation.py:62\u001b[39m, in \u001b[36m_validate_pyspark_installation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m+databricks.connect.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m version:\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# custom pyspark bundled with databricks-connect\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m     63\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyspark and databricks-connect cannot be installed at the same time. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTo use databricks-connect, uninstall databricks-connect & pyspark by running \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mpip uninstall -y databricks-connect pyspark pyspark-connect pyspark-client\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfollowed by a re-installation of databricks-connect\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m )\n",
      "\u001b[31mException\u001b[39m: pyspark and databricks-connect cannot be installed at the same time. To use databricks-connect, uninstall databricks-connect & pyspark by running 'pip uninstall -y databricks-connect pyspark pyspark-connect pyspark-client' followed by a re-installation of databricks-connect"
     ]
    }
   ],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "# This uses the environment variables from devcontainer\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5786eae9-c38e-4f3f-b1ec-4d2dc47c2270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/NAZ_Measure/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Matching configs:   0%|          | 0/3 [00:00<?, ?config/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from datetime import date\n",
    "import os\n",
    "import pandas as pd\n",
    "import streamlit as st  # only needed if you want Streamlit messages\n",
    "\n",
    "# -----------------------------\n",
    "# Add repo root to sys.path\n",
    "# -----------------------------\n",
    "repo_root = Path(\"/workspaces/NAZ_Measure\")  # adjust if needed\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import window\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "# -----------------------------\n",
    "# Configs / Inputs\n",
    "# -----------------------------\n",
    "from configs.shuffleboard_2025_config import (\n",
    "    start_date, \n",
    "    end_date, \n",
    "    measure_start, \n",
    "    measure_end,\n",
    "    brand_cluster_code, \n",
    "    desired_premise, \n",
    "    desired_retailer_channel,\n",
    "    vpid_offset_weeks, \n",
    "    match_configs,\n",
    ")\n",
    "from utils.data import get_raw_data_tables\n",
    "from utils.matching_algo import run_matching_variable_timing\n",
    "\n",
    "# -----------------------------\n",
    "# Spark session (secure)\n",
    "# -----------------------------\n",
    "def get_spark():\n",
    "    host = os.environ.get(\"DATABRICKS_HOST\")\n",
    "    token = os.environ.get(\"DATABRICKS_TOKEN\")\n",
    "    cluster_id = os.environ.get(\"DATABRICKS_CLUSTER_ID\")\n",
    "\n",
    "    if not host or not token or not cluster_id:\n",
    "        raise RuntimeError(\"Databricks environment variables not set!\")\n",
    "\n",
    "    try:\n",
    "        return DatabricksSession.builder \\\n",
    "            .host(host) \\\n",
    "            .token(token) \\\n",
    "            .cluster_id(cluster_id) \\\n",
    "            .getOrCreate()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to create Databricks session: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Timer start\n",
    "# -----------------------------\n",
    "t0 = perf_counter()\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare VPID DataFrame\n",
    "# -----------------------------\n",
    "vpid_timing_df = pd.DataFrame({\n",
    "    \"VPID\": list(vpid_offset_weeks.keys()),\n",
    "    \"identifier\": list(vpid_offset_weeks.values()),  \n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# Start Spark\n",
    "# -----------------------------\n",
    "spark = get_spark()\n",
    "\n",
    "# -----------------------------\n",
    "# Load raw tables\n",
    "# -----------------------------\n",
    "hv, hp, hr, hc = get_raw_data_tables(spark)\n",
    "\n",
    "# -----------------------------\n",
    "# Run matching\n",
    "# -----------------------------\n",
    "config_summary, matched_dfs, group_dfs = run_matching_variable_timing(\n",
    "    vpid_timing_df=vpid_timing_df,\n",
    "    match_configs=match_configs,\n",
    "    hv=hv, hp=hp, hr=hr, hc=hc,\n",
    "    base_start=start_date, base_end=end_date,\n",
    "    base_measure_start=measure_start, base_measure_end=measure_end,\n",
    "    brand_cluster_code=brand_cluster_code,\n",
    "    desired_premise=desired_premise,\n",
    "    desired_retailer_channel=desired_retailer_channel,\n",
    "    offset_col=\"identifier\", \n",
    "    max_controls_per_test=1,\n",
    "    data_end_cap=date(2025, 11, 30),\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Timer end & display\n",
    "# -----------------------------\n",
    "elapsed = perf_counter() - t0\n",
    "print(f\"run_matching_variable_timing runtime: {elapsed:.2f} seconds\")\n",
    "\n",
    "# Optional: if running in notebooks\n",
    "try:\n",
    "    display(config_summary)\n",
    "except NameError:\n",
    "    print(config_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735b6a70-afd5-493a-8c93-b924b28c7408",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770747144954}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# select * from vip_dev.retailer.vip_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3f63a5b-b11a-4e6d-9c56-79adcbdf5182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## View Individual Weeks Within a Config and Final DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03c31c0-a77f-48b4-a886-2f1121f4226a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763741478189}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cfg_name = \"minmax_all_blocking\"\n",
    "offset = 1   \n",
    "\n",
    "df = matched_dfs.get((cfg_name, offset))\n",
    "if df is not None:\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No matched_df for this config/offset\")\n",
    "dfs = []\n",
    "for i in range(0, 14):\n",
    "    df = group_dfs[(\"minmax_all_blocking\", i)].copy()\n",
    "    df[\"identifier\"] = i\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all_pairs = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "display(df_all_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0e27c6-06fa-47f4-a085-7ab2f7609556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Variable Timing Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3ad4cd-de0a-448e-a2e0-3c1b6a110a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m signature\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvalidate\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_raw_data_tables\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from collections import Counter\n",
    "from importlib import reload\n",
    "from inspect import signature\n",
    "import pandas as pd\n",
    "import utils.validate as validate\n",
    "from utils.data import get_raw_data_tables\n",
    "from pyspark.sql import SparkSession\n",
    "from configs.shuffleboard_2025_config import (\n",
    "    start_date, \n",
    "    end_date, \n",
    "    measure_start, \n",
    "    measure_end,\n",
    "    vpid_offset_weeks, \n",
    "    match_configs\n",
    ")\n",
    "\n",
    "\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "hv, hp, hr, hc = get_raw_data_tables(spark)\n",
    "target_id_counts = dict(Counter(vpid_offset_weeks.values()))\n",
    "iter_df = validate.run_validation_iterations_variable_timing(\n",
    "    vpid_timing_df=vpid_timing_df,\n",
    "    match_configs=match_configs,\n",
    "    hv=hv, hp=hp, hr=hr, hc=hc,\n",
    "    base_start=start_date, base_end=end_date,\n",
    "    base_measure_start=measure_start, base_measure_end=measure_end,\n",
    "    sample_size_total=175,\n",
    "    n_iterations=100,\n",
    "    rng_seed=42,\n",
    "    show_progress=True,\n",
    "    offset_col=\"identifier\",\n",
    "    target_id_counts=target_id_counts,\n",
    ")\n",
    "\n",
    "print(f\"iterations x configs rows: {len(iter_df)}\")\n",
    "display(iter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e59dd29e-58a9-475f-8697-94b516d1fd57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.data import build_variable_timing_caches\n",
    "\n",
    "# Build the same pre/post period caches\n",
    "offsets = sorted(vpid_timing_df[\"identifier\"].astype(int).unique().tolist())\n",
    "vol_pre_period_by_offset, vol_post_period_by_offset = build_variable_timing_caches(\n",
    "    hv, hp, hr, hc,\n",
    "    offsets=offsets,\n",
    "    base_start=start_date, base_end=end_date,\n",
    "    base_measure_start=measure_start, base_measure_end=measure_end,\n",
    "    vpids_for_post_period=None,\n",
    ")\n",
    "\n",
    "tests_requested_by_offset = {\n",
    "    k: set(vpid_timing_df.loc[vpid_timing_df[\"identifier\"] == k, \"VPID\"])\n",
    "    for k in offsets\n",
    "}\n",
    "\n",
    "tests_with_pre_by_offset = {\n",
    "    k: set(vol_pre_period_by_offset[k][\"VPID\"])\n",
    "    for k in offsets\n",
    "}\n",
    "\n",
    "lost_before_matching = {\n",
    "    k: tests_requested_by_offset[k] - tests_with_pre_by_offset[k]\n",
    "    for k in offsets\n",
    "}\n",
    "\n",
    "print(\"=== Missing from pre-period data (before matching) ===\")\n",
    "total_lost_before = 0\n",
    "for k in offsets:\n",
    "    n_lost = len(lost_before_matching[k])\n",
    "    total_lost_before += n_lost\n",
    "    if n_lost:\n",
    "        print(f\"Offset {k}: {n_lost} tests missing from pre-period data\")\n",
    "\n",
    "print(f\"TOTAL missing from pre-period data across all offsets: {total_lost_before}\\n\")\n",
    "\n",
    "cfg_name = match_configs[0].get(\"minmax_all_blocking\", list(match_configs[0].values())[0])  # fallback to first available key\n",
    "\n",
    "tests_in_result_by_offset = {\n",
    "    k: set(\n",
    "        group_dfs[(cfg_name, k)]\n",
    "        .query(\"Group == 'Test'\")[\"VPID\"]\n",
    "        .tolist()\n",
    "    )\n",
    "    for k in offsets\n",
    "}\n",
    "\n",
    "lost_in_matching = {\n",
    "    k: (tests_with_pre_by_offset[k] & tests_requested_by_offset[k]) - tests_in_result_by_offset[k]\n",
    "    for k in offsets\n",
    "}\n",
    "\n",
    "print(\"=== Lost during matching (have pre-period data but not in final membership) ===\")\n",
    "total_lost_matching = 0\n",
    "for k in offsets:\n",
    "    n_lost = len(lost_in_matching[k])\n",
    "    total_lost_matching += n_lost\n",
    "    if n_lost:\n",
    "        print(f\"Offset {k}: {n_lost} tests lost in matching\")\n",
    "\n",
    "print(f\"TOTAL lost in matching across all offsets: {total_lost_matching}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b03ee604-ac01-4e12-bc6c-3d77a7279321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## View Iteration Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf0a809-7383-4a49-879f-ac37d82cc292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "cfg_name = \"minmax_CYTrendShare_blocking\"\n",
    "\n",
    "groups = []\n",
    "for key, gdf in group_dfs.items():\n",
    "    if isinstance(key, tuple):\n",
    "        name, offset_weeks = key\n",
    "    else:\n",
    "        name, offset_weeks = key, None\n",
    "\n",
    "    if name != cfg_name or gdf is None or (isinstance(gdf, pd.DataFrame) and gdf.empty):\n",
    "        continue\n",
    "\n",
    "    tmp = gdf.copy()\n",
    "    if \"offset_weeks\" not in tmp.columns:\n",
    "        tmp[\"offset_weeks\"] = offset_weeks\n",
    "    groups.append(tmp[[\"Group\",\"VPID\",\"offset_weeks\"]])\n",
    "\n",
    "membership = pd.concat(groups, ignore_index=True) if groups else pd.DataFrame(columns=[\"Group\",\"VPID\",\"offset_weeks\"])\n",
    "membership = membership.drop_duplicates([\"Group\",\"VPID\",\"offset_weeks\"]).sort_values([\"offset_weeks\",\"Group\",\"VPID\"])\n",
    "print(f\"Membership rows for {cfg_name}: {len(membership)}\")\n",
    "display(membership)\n",
    "\n",
    "if \"offset_weeks\" in membership.columns:\n",
    "    display(membership.groupby([\"offset_weeks\",\"Group\"], as_index=False).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac904fd-edec-4113-b27b-538f248a7bdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39692dfb-5ad9-4ed5-8c45-0238b471c0d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.validate import run_delta_abs_tests_less\n",
    "\n",
    "\n",
    "tests = run_delta_abs_tests_less(iter_df)\n",
    "display(tests)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8873452893177810,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "us_fancy_matching_variable_timing",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
